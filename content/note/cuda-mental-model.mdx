---
title: 'CUDA mental model'
description: 'Quick mental model explanation for programming a GPU'
authors: ['romaingrx']
status: 'published'
published_date: 2025-10-13
tags: ['machine-learning', 'cuda']
resources:
  - title: 'Code snippet'
    type: 'code'
    provider: 'github'
    value: 'https://github.com/romaingrx/ml-notebooks/blob/master/gpu/cuda/01_indexing.cu'
    description: 'The full code snippet for the code example'
---

When I coded a CUDA kernel for the first time, I remember being a bit confused with the hierarchy and different levels (grid, blocks, ...). The goal of this note is to give a few helper functions and the overall mental model for the many dimensions.

# Grid, blocks, warps, threads, ...?

The goal of the [Single instruction, multiple threads (SIMT)](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads) model is to expose the same instruction, parallelize it as much as possible on the different parts of the data. CUDA uses a hierarchical structure where threads are organized into blocks, and blocks are organized into a grid. The configuration of this structure is left to the developer to make effective use of the hardware for the specific data shape and type of algorithm to run.

## Grid

A grid is the entire collection of parallel work for your kernel.

- It's composed of independent blocks that execute concurrently with indeterminate order
- A grid can be 1, 2 or 3 dimensional

## Block (thread block)
A block is a group of threads that executes on the same [streaming multiprocessor (SM)](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor).

- Threads in a block can communicate via [shared memory](https://modal.com/gpu-glossary/device-software/shared-memory) and synchronize using barriers
- A block contains one or more warps, where a warp is a constant number of threads (`32` for all Nvidia GPUs so far) that run at once
- A block can also be 1, 2 or 3 dimensional

## Thread

This is the individual unit of execution.

- The role is to access the right part of the data based on its index and execute the kernel


# Code example

The easiest way to explore this is to actually log the index of each thread. To determine the final index of the thread, we need to get:

1. The offset of the current block in the grid
2. The offset of the current thread in the current block
3. The sum of all the offsets to obtain the global thread ID

To do so, I've implemented some easy helper functions to unroll all these indices in a comprehensive way.

## `get_offset`

The first function `get_offset` simply gets the flattened index of the `coordinates` variable in a 3 dimensional cube of dimensions `dim`. For example, if we're at the coordinates `dim3(2, 2, 2)` in a cube of dimension `dim3(8, 8, 8)`, we need to sum all the individual units that came before our current `coordinates` in the following way:
1. The current index in the x direction which is simply `coordinates.x`
2. The current index in the y direction, which means that we need to take into account all the `coordinates.y` number of "rows" of dimensions `dim.x`
3. And finally sum all the "rows" and "cols" of dimensions `dim.x` and `dim.y` times our coordinate in direction z `coordinates.z`

```cu
__device__ int get_offset(dim3 coordinates, dim3 dim){
  return
    coordinates.x +
    coordinates.y * dim.x +
    coordinates.z * dim.x * dim.y;
}
```

So in our cube example of dimensions `dim3(8, 8, 8)`, we'll have a flattened index that can go between $0$ and $512$ ($8*8*8$).

## Main function `whoami`

In CUDA, we define our kernel with a `__global__` keyword and in our kernel we have access to the following variables:
- `gridDim` which defines the 3D dimensions of our grid
- `blockDim` which also defines the dimensions of our blocks within the grid
- `blockIdx` is the coordinate of the current block we're in
- `threadIdx` is the coordinate of the current thread within the current block

All these variables are 3 dimensional (`dim3`).

So if we decide on our grid dimension and block dimensions, we'll be able to observe all our thread indexes within the blocks and grid.


```cu title=01_indexing.cu
#include <stdio.h>

// Simple helper function that linearizes the id
__device__ int get_offset(dim3 idx, dim3 dim){
  return
    idx.x +
    idx.y * dim.x +
    idx.z * dim.x * dim.y;
}

__global__ void whoami() {
  // Defines the flattened id of the block inside the grid
  int block_offset = get_offset(blockIdx, gridDim);

  // We got the offset in block units, now we need to account for all the threads that were in the previous blocks to have the offset in thread units
  int thread_block_offset =
    block_offset
    * blockDim.x * blockDim.y * blockDim.z;

  // Defines the offset of the thread inside the current block
  int thread_offset = get_offset(threadIdx, blockDim);

  // Final id is the sum of how many threads came before + the thread offset inside the current block
  int id = thread_block_offset + thread_offset;

  printf("%04d | Block(%d %d %d) = %3d | Thread(%d %d %d) = %3d\n",
        id,
        blockIdx.x, blockIdx.y, blockIdx.z, block_offset,
        threadIdx.x, threadIdx.y, threadIdx.z, thread_offset);
}

int main() {
  dim3 blocksPerGrid(2, 2, 2);
  dim3 threadsPerBlock(2, 1, 1);

  int blocks_per_grid = blocksPerGrid.x * blocksPerGrid.y * blocksPerGrid.z;
  int threads_per_block = threadsPerBlock.x * threadsPerBlock.y * threadsPerBlock.z;
  printf("%d blocks/grid\n", blocks_per_grid);
  printf("%d threads/block\n", threads_per_block);
  printf("%d total threads\n", blocks_per_grid * threads_per_block);

  whoami<<<blocksPerGrid, threadsPerBlock>>>();
  cudaDeviceSynchronize();

  return 0;
}
```

This code will give us a result similar to the following (although the order of the thread execution is not guaranteed)

```zsh
‚ùØ nvcc 01_indexing.cu && ./a.out
8 blocks/grid
2 threads/block
16 total threads
0000 | Block(0 0 0) =   0 | Thread(0 0 0) =   0
0001 | Block(0 0 0) =   0 | Thread(1 0 0) =   1
0002 | Block(1 0 0) =   1 | Thread(0 0 0) =   0
0003 | Block(1 0 0) =   1 | Thread(1 0 0) =   1
0004 | Block(0 1 0) =   2 | Thread(0 0 0) =   0
0005 | Block(0 1 0) =   2 | Thread(1 0 0) =   1
0006 | Block(1 1 0) =   3 | Thread(0 0 0) =   0
0007 | Block(1 1 0) =   3 | Thread(1 0 0) =   1
0008 | Block(0 0 1) =   4 | Thread(0 0 0) =   0
0009 | Block(0 0 1) =   4 | Thread(1 0 0) =   1
0010 | Block(1 0 1) =   5 | Thread(0 0 0) =   0
0011 | Block(1 0 1) =   5 | Thread(1 0 0) =   1
0012 | Block(0 1 1) =   6 | Thread(0 0 0) =   0
0013 | Block(0 1 1) =   6 | Thread(1 0 0) =   1
0014 | Block(1 1 1) =   7 | Thread(0 0 0) =   0
0015 | Block(1 1 1) =   7 | Thread(1 0 0) =   1
```
