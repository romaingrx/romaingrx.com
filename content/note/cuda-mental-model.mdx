---
title: 'CUDA mental model'
description: 'Quick mental model explanation for programming a GPU'
authors: ['romaingrx']
status: 'draft'
published_date: 2025-10-12
tags: ['machine-learning', 'cuda']
resources:
  - title: 'Code snippet'
    type: 'code'
    provider: 'github'
    value: 'https://github.com/romaingrx/ml-notebooks/blob/master/gpu/cuda/01_indexing.cu'
    description: 'The full code snippet for the code example'
---

When I coded a CUDA kernel for the first time, I remember being a bit confused with the hierarchy and different levels (grid, blocks, ...). The goal of this note is to give a few helper functions and the overall mental model over the many dimensions.

# Grid, blocks, warps, threads, ...?

The goal of the [Single instruction, multiple threads (SIMT)](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads) model is to expose the same instruction, parallelize it as much as possible on the different part of the data. CUDA uses a hierarchical structure where threads are organized into blocks, and blocks are organized into a grid. The configuration of this structure is left to developper to make effective use of the hardware for the specific data shape and type of algorithm to run.

## Grid

A grid is the entire collection of parallel work for your kernel to do the work

- It's composed of independant blocks that execute concurrently with indeterminated order
- A grid can be 1, 2 or 3 dimensionnal

## Block (thread block)
A block is a group of threads that executes on the same [streaming multiprocessor (SM)](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor).

- Threads in a block can communicate via [shared memory](https://modal.com/gpu-glossary/device-software/shared-memory) and synchronize using barriers
- A block contains one or more warps, where a warp is a constant number of threads (`32` for all Nvidia gpus so far) that run at once
- A block can be as well 1, 2 or 3 dimensionnal

## Thread

This is the individual unit of execution.

- The role is to access the right part of the data based on its index and execute the kernel


# Code example

The easiest to explore this is to actually log the index of each thread. To determine the final index of the thread, we need to get:

1. The index of the current block in the grid
2. The index of the current thread in the current block
3. The sum of all the offsets to obtain the global thread id

To do so, I've implemented some easy helper functions to unroll all these indices in a comprehensive way.

## `get_id`

The first function `get_id` simply gets the flattened index of the `coordinates` variable in a 3 dimensionnal cube of dimensions `dim`. Effectively if we're at the coordinates `dim3(2, 2, 2)` in a cube of dimension `dim3(8, 8, 8)`, we need to effectively sum all the individual units that came before our current `coordinates` in the following way:
1. The current index in the x direction which is simply `coordinates.x`
2. The current index in the y direction, which means that we need to take into account all the `coordinates.y` number of "rows" of dimensions `dim.x`
3. And finally sum all the "rows" and "cols" of dimensions `dim.x` and `dim.y` times our coordinate in direction z `coordinates.z`

```cuda
__device__ int get_id(dim3 coordinates, dim3 dim){
  return
    coordinates.x +
    coordinates.y * dim.x +
    coordinates.z * dim.x * dim.y;
}
```

So in our cube example of dimensions `dim3(8, 8, 8)`, we'll have a flattened index that can go between $0$ and $512$ ($8*8*8$).

## Main function `whoami`

In CUDA, we define our kernel with a `__global__` keyword and in our kernel we have access to the following variables:
- `gridDim` which defines the 3d dimensions of our grid
- `blockDim` which also defines the dimensions of our blocks within the grid
- `blockIdx` is the coordinate of the current block we're in
- `threadIdx` is the coordinate of the current thread within the current block

All these variables are 3 dimensional (`dim3`).

So if we decide on our grid dimension and block dimensions, we'll be able to observe all our threads indexes within the blocks and grid.


```cuda
#include <stdio.h>

// Simple helper function that linearizes the id
__device__ int get_id(dim3 idx, dim3 dim){
  return
    idx.x +
    idx.y * dim.x +
    idx.z * dim.x * dim.y;
}

__global__ void whoami() {
  // Defines the flattened id of the block inside the grid
  int block_id = get_id(blockIdx, gridDim);

  // Defines how many thread came before that block
  int block_offset = 
    block_id
    * blockDim.x * blockDim.y * blockDim.z;

  // Defines the id of the thread inside the block
  int thread_offset = get_id(threadIdx, blockDim);

  // Final id is the sum of how many threads came before + the thread offset inside the current block
  int id = block_offset + thread_offset;

  printf("%04d | Block(%d %d %d) = %3d | Thread(%d %d %d) = %3d\n",
        id,
        blockIdx.x, blockIdx.y, blockIdx.z, block_id,
        threadIdx.x, threadIdx.y, threadIdx.z, thread_offset);
}

int main() { 
  dim3 blocksPerGrid(2, 2, 2);
  dim3 threadsPerBlock(2, 1, 1);

  int blocks_per_grid = blocksPerGrid.x * blocksPerGrid.y * blocksPerGrid.z;
  int threads_per_block = threadsPerBlock.x * threadsPerBlock.y * threadsPerBlock.z;
  printf("%d blocks/grid\n", blocks_per_grid);
  printf("%d threads/block\n", threads_per_block);
  printf("%d total threads\n", blocks_per_grid * threads_per_block);

  whoami<<<blocksPerGrid, threadsPerBlock>>>();
  cudaDeviceSynchronize();

  return 0;
}
```

This code will give us a result similar to the following (although the order of the threads execution is not guaranteed)

```zsh
‚ùØ nvcc 01_indexing.cu && ./a.out
8 blocks/grid
2 threads/block
16 total threads
0000 | Block(0 0 0) =   0 | Thread(0 0 0) =   0
0001 | Block(0 0 0) =   0 | Thread(1 0 0) =   1
0002 | Block(1 0 0) =   1 | Thread(0 0 0) =   0
0003 | Block(1 0 0) =   1 | Thread(1 0 0) =   1
0004 | Block(0 1 0) =   2 | Thread(0 0 0) =   0
0005 | Block(0 1 0) =   2 | Thread(1 0 0) =   1
0006 | Block(1 1 0) =   3 | Thread(0 0 0) =   0
0007 | Block(1 1 0) =   3 | Thread(1 0 0) =   1
0008 | Block(0 0 1) =   4 | Thread(0 0 0) =   0
0009 | Block(0 0 1) =   4 | Thread(1 0 0) =   1
0010 | Block(1 0 1) =   5 | Thread(0 0 0) =   0
0011 | Block(1 0 1) =   5 | Thread(1 0 0) =   1
0012 | Block(0 1 1) =   6 | Thread(0 0 0) =   0
0013 | Block(0 1 1) =   6 | Thread(1 0 0) =   1
0014 | Block(1 1 1) =   7 | Thread(0 0 0) =   0
0015 | Block(1 1 1) =   7 | Thread(1 0 0) =   1
```

